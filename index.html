<!DOCTYPE html>
<html lang="en-US">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Brooke Hudson - Portfolio</title>
    <link rel="stylesheet" href="css/style.css">
</head>

<body>
    <!-- make the main tab bar  -->
    <!-- make a secondary tab bar when hovering over a tab, dissapears when not hovering primary or secondary tab bar -->
    <tab class="tab">
        <div class="tab-set">
        <button class="tablinks" onclick="openTab(event, 'HomePage')" id="defaultOpenHomePage">Home Page</button>
        </div>
        <!-- <button class="tablinks" onclick="openTab(event, 'CVDrone')">Computer Vision Drone</button> -->
        <div class="tab-set">
            <button class="tablinks" onclick="openTab(event, 'SeniorDesign')">Traffic Object Detection (Senior Design)</button>
            <tab class="subtab" id="subtab">
                <button class="sublinks" onclick="openSubTab(event, 'SeniorDesign_Overview')" id="defaultOpenSeniorDesign">Project Overview</button>
                <button class="sublinks" onclick="openSubTab(event, 'SeniorDesign_Hardware&Electronics')">Hardware and Electronics</button>
                <button class="sublinks" onclick="openSubTab(event, 'SeniorDesign_CV')">Object Detection Implementation</button>
                <button class="sublinks" onclick="openSubTab(event, 'SeniorDesign_LoRaWAN')">LoRaWAN Data Transmission</button>
            </tab>
        </div>
        <div class="tab-set">
            <button class="tablinks" onclick="openTab(event, 'FastRobot')">Fast Robotics</button>
            <tab class="subtab" id="subtab2">
                <button class="sublinks" onclick="openSubTab(event, 'FastRobot_Overview')" id="defaultOpenFastRobot">Project Overview</button>
                <button class="sublinks" onclick="openSubTab(event, 'FastRobot_Lab1')">Lab 1</button>
                <button class="sublinks" onclick="openSubTab(event, 'FastRobot_Lab2')">Lab 2</button>
                <button class="sublinks" onclick="openSubTab(event, 'FastRobot_Lab3')">Lab 3</button>
                <button class="sublinks" onclick="openSubTab(event, 'RastRobot_Lab4')">Lab 4</button>
                <button class="sublinks" onclick="openSubTab(event, 'FastRobot_Lab5')">Lab 5</button>
                <button class="sublinks" onclick="openSubTab(event, 'FastRobot_Lab6')">Lab 6</button>
                <button class="sublinks" onclick="openSubTab(event, 'FastRobot_Lab7')">Lab 7</button>
                <button class="sublinks" onclick="openSubTab(event, 'RastRobot_Lab8')">Lab 8</button>
                <button class="sublinks" onclick="openSubTab(event, 'FastRobot_Lab9')">Lab 9</button>
                <button class="sublinks" onclick="openSubTab(event, 'RastRobot_Lab10')">Lab 10</button>
                <button class="sublinks" onclick="openSubTab(event, 'FastRobot_Lab11')">Lab 11</button>
        </div>
        <div class="tab-set">
            <button class="tablinks" onclick="openTab(event, 'MakeAThon')">Big Red Make-A-Thon</button>
            <tab class="subtab" id="subtab3">
                <button class="sublinks" onclick="openSubTab(event, 'MakeAThon_Overview')" id="defaultOpenMakeAThon">Project Overview</button>
                <button class="sublinks" onclick="openSubTab(event, 'MakeAThon_Tools')">PLACEHOLDER</button>
        </div>
    </tab>


    <!-- make the tab content -->
    <div id="HomePage" class="tabcontent">
        <!-- Add scrolling photo bar -->
         <div class="static-image-container" style="top:0px;">
         <div class="image-container">
            <img class="scroll_img" src="files/image_cycle_makeAThon.png">
            <img class="scroll_img" src="files/image_cycle_ithacaCommons.png">
            <img class="scroll_img" src="files/image_cycle_droneCloseUp.jpg">
            <img class="scroll_img" src="files/image_cycle_labPhoto.png">
            <img class="scroll_img" src="files/image_cycle_makeAThon.png">
            <img class="scroll_img" src="files/image_cycle_ithacaCommons.png">
            <img class="scroll_img" src="files/image_cycle_droneCloseUp.jpg">
            <img class="scroll_img" src="files/image_cycle_labPhoto.png">
         </div>
         </div>
        <!-- About Me Section -->
         <div class="text-row" style="padding: 5vh 3vw;">
            <div class="left-column">
            <h2>About Me</h2>
                <div class="resume-link">
                <a href="files/2025Resume_BrookeHudson.pdf" download>Download My Resume</a>
                </div>
            </div>
            <div class="right-column">
            <p>I am a new graduate from Cornell University, having double majored in Computer Science and Mechanical Engineering. 
                I have been involved in a variety of projects, with interest in robotics (Cornell CUP robotics, Fast Robots project, 
                Big Red Make-a-thon) and some environmental science (senior design project, Cornell University Project Greenhouse, Cape Henlopen 
                State Park Internship). This portfolio chronicles several of these technical projects. This portfolio is also a bit or a 
                mini-project itself! No templates or bootstrap/ similar assistive tools were used in its creation: only HTML, CSS, and JavaScript. </p>
            </div>
            </div>
        <!-- Contact Information -->
         <div class="text-row" style="background-color: lightgoldenrodyellow;padding: 0vh 3vw;">
            <div class="left-column">
            <h2 style="color: black; font-size: 30px;">Contact Information</h2>
            </div>  
            <div class="right-column" style="float: left;">
                <div class="text-row" style="background-color: lightgoldenrodyellow;padding: 0px;">
                    <div class="left-column" style="background-color: lightgoldenrodyellow;width: fit-content;float: left;">
                    <a href="https://www.linkedin.com/in/brooke-hudson-658032228/" target="_blank">
                    <img id="linkedin" src="https://upload.wikimedia.org/wikipedia/commons/c/ca/LinkedIn_logo_initials.png" style="height:24px;width:auto;display:inline" alt="Linkedin icon" />
                    </a>
                    </div>
                    <div class="right-column" style="background-color: lightgoldenrodyellow;float: left;">
                    <p style="color: black;">bah253@cornell.edu</p>
                    </div>
            </div>
            </div>
        </div>
    </div>
    
    <div id="SeniorDesign_Overview" class="subtabcontent">
        <!-- title  -->
        <div style="background-color: black; margin: 10vh 98vw;"></div>
        <div style="background-color: black; width:98vw;">
            <h2 style="color:white;font-size: 30px;">Cornell University Senior Design Project</h2>
        <h2 style="padding:0.5% 0;">Traffic Tracking Using Object Detection</h2>
        </div>
        <!-- Add scrolling photo bar -->
         <!-- <div class="static-image-container">
         <div class="image-container" style="height: 50vh;animation-duration: 35s;">
            <img class="scroll_img" src="files/image_cycle2_seniorDesignReal.png">
            <img class="scroll_img" src="files/image_cycle2_seniorDesignTestResult.png">
            <img class="scroll_img" src="files/image_cycle2_seniorDesignSchematic.png">
            <img class="scroll_img" src="files/image_cycle2_seniorDesignTestSetup.png">
            <img class="scroll_img" src="files/image_cycle2_seniorDesignReal.png">
            <img class="scroll_img" src="files/image_cycle2_seniorDesignTestResult.png">
            <img class="scroll_img" src="files/image_cycle2_seniorDesignSchematic.png">
            <img class="scroll_img" src="files/image_cycle2_seniorDesignTestSetup.png">
        </div>
        </div> -->
        <!-- Project Overview -->
        <div class="text-image-background">
            <div class="color-image-background" style="width:98vw;">
                <img class="background-image" src="files/image_background_SrDesignScematic.png">
                <h2 style="color:black;font-size: 40px;position: absolute; top: 10%;"> Project Overview and Purpose</h2>
                <p style="color:black;position: absolute; top: 25%;width: 50%;">
                    For this project Cornell University partnered with a local environmental justice group "Blueprint Geneva." Blueprint 
                    Geneva addresses a variety of environmental concerns including general air quality, landfill pollution 
                    (a landfill is located in Geneva), and traffic pollution. The environmental group was not satisfied with their current 
                    method of traffic tracking, which requested high school volunteers to manually count the cars. Drawbacks of 
                    this method include a lack of data at times when high schoolers are not available (most of the school day everyday 
                    and most of the night) and human error in count totals and traffic type classification. Therefore, Blueprint Geneva 
                    requested a device that would track traffic. The students involved designed and tested a system 
                    that would use object detection/ computer vision to identify the cars and use LoRaWAN data transmission to send the data 
                    back for the environmental group to analyze. 
                </p>
            </div>
        </div>
        <h2 style="padding:1% 0;position: relative;text-align: center;">Quantitative Goals</h2>
        <div style="background-color: orangered;height: 0.5vh; width: 98vw;"></div>
        <div style="height:20vh" class="text-row">
            <div class="stat-block" style ="width:32vw;">
                <p>The device detects</p>
                <h2>13</h2>
                <p>different types of vehicles.</p>
            </div>
            <div class="stat-block-divider"></div>
            <div class="stat-block" style ="width:32vw;">
                <p>The cost per device is</p>
                <h2>$ 196.72</h2>
                <p>per traffic tracking unit.</p>
            </div>
            <div class="stat-block-divider"></div>
            <div class="stat-block" style ="width:32vw;">
                <p style="width:33vw;">Object Detection Uses <br>Ultralytics YOLO version</p>
                <h2 style="width:33vw;">8</h2>
                <div class="resume-link" style="width:fit-content;margin-left: auto; margin-right: auto;">
                    <a href="https://github.com/ultralytics/ultralytics" target="_blank">Learn More About Ultralytics YOLO</a></div>
            </div>
        </div>
    </div>

    <div id="SeniorDesign_Hardware&Electronics" class="subtabcontent">
        <div style="background-color: black; margin: 10vh 100vw;"></div>
        <h2 style="color:white;font-size: 30px;">Hardware and Electronics Selection</h2>
        <h2 style="padding:0.5% 0;">Traffic Tracking Using Object Detection</h2>
        <!-- Add scrolling photo bar -->
         <div class="static-image-container">
         <div class="image-container" style="height: 50vh;animation-duration: 35s;">
            <img class="scroll_img" src="files/image_cycle2_seniorDesignReal.png">
            <img class="scroll_img" src="files/image_cycle2_raspberryPi4.png">
            <img class="scroll_img" src="files/image_cycle2_seniorDesignSchematic.png">
            <img class="scroll_img" src="files/image_cycle2_seniorDesignTestSetup.png">
            <img class="scroll_img" src="files/image_cycle2_seniorDesignReal.png">
            <img class="scroll_img" src="files/image_cycle2_raspberryPi4.png">
            <img class="scroll_img" src="files/image_cycle2_seniorDesignSchematic.png">
            <img class="scroll_img" src="files/image_cycle2_seniorDesignTestSetup.png">
        </div>
        </div>
        <div class="text-row" style="width: 100%;height:40vw;">
            <div style="width: 50%;height: 100%;display:flex;align-items: center;justify-content: center;">
                <h2 style="color:white;">The Electronics</h2>
            </div>
            <div class="hover-image-container" style="width:50%;height:100%">
                <img src="files/image_electronics_schematic.png" alt="The electronics schematic includes a raspberry pi 4b, an arducam, an 
                M0 feather board with LoRaWAN data transmission capability, and a buck converter to the power supply." style="width:100%;">
                <!-- <div class="hover-block" style="
                    position: realtive;
                    top: 0;
                    width: 50%;
                    height: 50%;
                    background-color: white;">
                </div> -->
            </div>
        </div>
        <!-- Add BOM and Justification -->
        <div style="background-color: orangered;height: 0.5vh; width: 100%;padding:0"></div>
        <div style="width: 100vw;display:flex;flex-direction: row;padding:0">
            <div>
                <h2>Part Justification</h2>
            </div>
            <div class="dropdown-menu" style="position:absolute; left: 80%;margin:10px;" >
                <div class="dropdown-label">
                    <p>Select A Part</p>
                    <div class="dropdown-options">
                        <button class="dropbtn" onclick="openDropdownMenu(event, 'raspberryPi')">Raspberry Pi</button>
                        <button class="dropbtn" onclick="openDropdownMenu(event, 'camera&lens')">Arducam & Lens</button>
                        <button class="dropbtn" onclick="openDropdownMenu(event, 'featherM0Board')">AdaFruit Feather M0 Board</button>
                        <button class="dropbtn" onclick="openDropdownMenu(event, 'ACDCBuckConverter')">AC-DC Buck Converter</button>
                        <button class="dropbtn" onclick="openDropdownMenu(event, 'enclosure&switch')">Enclosure & Push Switch</button>
                    </div>
                </div>
            </div>
        </div>
        <div class="dropdowncontent" id="no-content">
            <div style="width:100vw;height:30vh;background-color: black;"></div>
        </div>
        <div class="dropdowncontent" id="raspberryPi">
            <h3>Raspberry Pi 4B</h3>
            <div class="text-row">
                <div class="left-column">
                    <p>The team first attempted to use a Raspberry Pi Zero 2 W. This has 512 MB of RAM, while the object detection algorithm 
                        needs ~450 MB of available RAM without including RAM needed for the photo storage itself (each photo frame takes 
                        10-15 MB of storage). This resulted in limited performance from the Zero W 2 during initial tests. The Pi would freeze 
                        for long periods of time (not ideal for a real-time object detection use case) or crash and need to be rebooted (not ideal 
                        for almost ~any~ use case).
                    </p>
                </div>
                <div class="right-column" style="display: inline-flex;justify-content: center; align-items:center; flex-direction: column;">
                    <img src="files/image_raspberryPi02W.png" alt="Raspberry Pi Zero W 2 board" style="width: 60%;">
                    <p style="text-decoration: wavy; font-size:15px">A raspberry Pi Zero W 2</p>
                </div>
            </div>
            <div class="text-row">
                <div class="left-column" style="width:55vw;display: inline-flex;justify-content: center;align-items:center;flex-direction: column;">
                    <img src="files/image_raspberryPi4B.png" alt="Raspberry Pi 4B board" style="width: 60%;">
                    <p style="text-decoration: wavy; font-size:15px">A raspberry Pi 4B</p>
                </div>
                <div class="right-column" style="width:40vw;padding:0vh 1vw;">
                    <p>The team then switched to a Raspberry Pi 4B, which has 8 GB of RAM. (While some Pi 4s have different storage amounts and 
                        this could be somewhat overkill, this was the only Pi available to the team with sufficient memory). This allowed the 
                        object detection algorithm to run without pushing its memory limits. 
                    </p>
                </div>
        </div>

        </div>
        <div class="dropdowncontent" id="camera&lens">
            <h3>Arducam Camera and Lens</h3>
            <div class="text-image-background" style="width:100vw;height:15vh;">
                <div class="color-image-background" style="width: 100%;
                height: fit-content;
                position: relative;
                background-color: black;">
                    <img class="background-image" src="files/image_arducam_background.png" 
                    alt="The Arducam camera and lens used for the object detection system."
                    style="mask-image: linear-gradient(
                        to left, white, transparent 90%
                      );">
                    <p style="color:white;position: absolute; top: 25%;width: 50%;">Our chosen camera: the Arducam for Raspberry Pi HQ Camera Module, 12.3MP IMX477 has adjustable focus and zoom (8-50mm)
                        since the community partner has indicated that they intend for 
                        the unit to be used in multiple locations. The main drawback of this method is that the camera video feed 
                        can only be seen while the device is attached to a computer monitor, which means initial setup will require a 
                        computer monitor onsite. This is an inconvenient and awkward setup, but this is only required for the first use.
                    </p>
                </div>
                
        </div>
        </div>
        <div class="dropdowncontent" id="featherM0Board">
            <h3>AdaFruit Feather M0 Board with LoRaWAN</h3>
            <div class="text-image-background">
                <div class="color-image-background" style="width: 100%;
                height: fit-content;
                position: relative;
                background-color: black;">
                    <img class="background-image" src="files/image_featherM0_background.png"
                    alt="The AdaFruit Feather M0 board with RFM95 LoRaWAN data transmission capabilities."
                    style="mask-image: linear-gradient(
                        to left, white, transparent 70%
                      );">
                    <p style="color:white;position: absolute; top: 25%;width: 50%;">
                        The community partner specified that the device may be used in areas without WiFi; however, the greater Ithaca and 
                        Geneva area have LoRaWAN coverage. Therefore, the AdaFruit Feather M0 board with RFM95 LoRaWAN data transmission 
                        was chosen to transmit the data back to the community partner. Additionally, the board had been used by all team members 
                        involved in the project in a series of training labs. The team used The Things Network (TTN) to upload the data 
                        from the board to the dashboard. Each packet included 13 vehicle ids, and the number of vehicles detected with each id.
                        For example, sensing one car (vehicle id 0) would result in a packet like this: <br>
                        {vehicle id 0: 1, vehicle id 1: 0, . . . vehicle id 12: 0}. <br>
                        The diagram below demonstrates a successful transmission test of a LoRaWAN packet:
                    </p>
                </div>
            </div>
            <img src="files/image_lorawan_packet.png" alt="A successful LoRaWAN packet transmission test from the AdaFruit Feather M0 board." style="width: 100%;">
        </div>
        <div class="dropdowncontent" id="ACDCBuckConverter">
            <h3>AC-DC Buck Converter</h3>
            <p style="color:white">
                North American power outlets provide 120V of power, which far exceeds the 5V needed for this device. 5V is necessary since this 
                is standard for the Rasperry Pi being used. Therefore, the Digikey RS-15-5 converter fit the project's needs. The buck converter 
                has live, neutral, and ground ports for connetion to the powerline through an AC power cord.
            </p>
        </div>
        <div class="dropdowncontent" id="enclosure&switch">
            <h3>Enclosure and Push Switch</h3>
            <div class="text-row" style="width: 100%;height:30vh;position:absolute;">
                <div style="width: 50%;height: 100%;display:flex;align-items: center;justify-content: center;">
                    <p style="color:white;">
                        The enclosure for the device was chosen to be a waterproof enclosure, since the community partner indicated that they would like 
                        to use the device outdoors. This makes sense in order to capture traffic patterns on roads which are outdoors.
                        The enclosure has a clear front panel, which allows the camera to see through it. The push switch 
                        was chosen to be a waterproof push switch. It is connected in series with the line or 
                        neutral inputs to the buck converter. A small hole was drilled in the enslosure so that the switch is accessible 
                        from the outside of the box. 
                    </p>
                </div>
                <img src="files/image_enclosure_pushSwitch.png" alt="The waterproof enclosure and push switch." style="width:50%;">
            </div>
         </div>
        </div>


    <div id="SeniorDesign_CV" class="subtabcontent">
        <div class="flashing-image-container" style="height:60vh;">
            <img src="files/image_cv_exmaple.png" alt="The object detection successfully identifies cars and busses."
                style="animation-delay: 6s;opacity:0;">
            <img src="files/image_cv_exmaple2.png" alt="The object detection successfully identifies cars and busses." 
                style="animation-delay: 3s;opacity:0;">
            <img src="files/image_cv_exmaple3.png" alt="The object detection successfully identifies cars and busses." 
                style="width:100%;">
        </div>
        <h2>Overall Code Flow</h2>
        <div style="padding:0;margin:0;width:95vw;overflow:hidden;">
            <img src="files/image_cvCodeFlow.png" alt="The overall code flow for the object detection algorithm." style="width:100%;padding:0;margin:0;">
        </div>
        <div style="padding:0;;margin:2vw;width:88vw;overflow:hidden;">
            <h2>Software Version Choices</h2>
            <div class="resume-link" style="width:fit-content;">
                <a href="https://github.com/ultralytics/ultralytics" target="_blank">Learn More About Ultralytics YOLO</a></div>
            <p>When training the original object detection model, the team used the most updated version (Yolo v11). This would not run at all on 
                the raspberry pi, so the team attempted to revert to an older version. Upon using the less computationally expensive Yolo v5, 
                the dependencies were not compatable with current OpenCV commands. Therefore, the team used Yolo v8 to train the object detection model. 
                Version 8 would run on the raspberry pi and was compatable with the OpenCV commands. 
            </p>
        </div>
        <div style="background-color: black; width:80vw; height: 5vh;"></div>
        <h2 style="background-color:white;margin-bottom:2px;width: 100%;">Raspberry Pi Compatability with OpenCV</h2>
        <div class="text-row" style="background-color:white;overflow:hidden;">
            <div class="left-column" style="width:50vw;border-right: 2px black solid;">
                <p style="background-color:white;color:black;">
                    OpenCV was used for analysing the images taken on the pi camera. However, the raspberry pi operating system used was Bookworm 
                    (the most updated available Rasberry Pi OS). Bookworm uses a camera module called libcamera, while the older OS Bullseye 
                    uses the Legacy Camera Module. OpenCV is compatable with the older raspberry pi OS Bullseye/ the Legacy Camera Module, 
                    but not with the more recent OS Bookworm/ libcamera. The original code included the simple command "ret, frame = cap.read()" 
                    or "cv2.videoCapture(camera_number)"to 
                    capture a frame and feed the frame 
                    variable into OpenCV functions. This command uses the legacy camera module.
                    The solution to this incompatability was to create a function that would use libcamera to capture the frame and then convert 
                    the libcamera frame into a format comaptable with OpenCV. The function created is shown below.
                </p>
                <div style="background-color: white; width:100%; height: 5vh;"></div>
                <p style="background-color:white;color:black;">Additionally, a flag was put in this code incase future student groups in contact with Blueprint Geneva continue working on the device.
                    For ease of use with any camera module, the flag "new_rasos" can be set to 1 (to use the new function) or 0 for use with the 
                    Legacy Camera Module. If an older raspberry pi or another computing device instead of a pi is used, the code can still function.
               </p>
            </div>
            <img src="files/image_getFrameFunc.png" alt="The code block used to make the libcamera photo compatable with OpenCV." style="width:47vw;padding:0;margin:0;">
        </div>

    </div>
    <div id="SeniorDesign_LoRaWAN" class="subtabcontent">
        
    </div>

    <div id="FastRobot_Overview" class="subtabcontent">
        <div class="flashing-image-container" style="height:90vh; width:98vw;overflow:hidden;">
            <!-- <video class="video1" height="90%" autoplay loop muted>
                <source src="files/videos/fastrobot_overview1.mp4">
            </video> -->
            <video class="video2" height="90%" autoplay loop muted>
                <source src="files/videos/fastrobot_overview2.mp4">
            </video>
            <!-- <video class="video3" height="90%" autoplay loop muted>
                <source src="files/videos/fastrobot_overview3.mp4">
            </video> -->
        </div>
        <div style="position:relative; width:98vw;">
            <h2>Project Overview</h2>
            <p>
                This project was made under professor guidance for a class titled "Fast Robots." These labs, unlike the other projects, are not group 
                projects. The labs from this class have been included. This is the most dense project description in the portfolio, so the 
                highlights and skills are outlined here in order to point out relevant sections to any given user. 
                The technical skills highlighted in this lab include: soldering skills and electronics integration (labs 2-4), PID implementation (labs 5-6), 
                kalman filtering (lab 7), and localization (lab 9-11).
            </p>
        </div>
    </div>
    <div id="FastRobot_Lab1" class="subtabcontent">
        <div style="background-color: black; margin: 10vh 100vw;"></div>
        <h2 style="color:white;">Lab1</h2>
        <h3>Prelab : Setup</h3>
        <p>
            It was necessary to first download the Arduino IDE (ver. 2.3.4) 
            and Artemis Nano board extension. A virtual environment was also created for editing files in python and 
            JupyterNotebook in order to send and recieve messages from the board with Bluetooth Low Energy (BLE). Then the Lab1A tasks 
            and scripts were run to verify that the communication between devices worked without bluetooth and that the Artemis board was 
            functioning properly. Then Lab 1B involved the virtual environment, which was used with more given and modified scripts focused on 
            bluetooth use. Before working on lab tasks and working with the bluetooth, the MAC adress from the Artemis boad was: </p>
        <img src="files/FastRobots/Lab1/mac_address.png">
        <p>
            The UUID (Universally Unique Identifier) was generated as well to avoid confusion with other boards in the room. The following block 
            includes the input lines and output (ie the generated UID).
        </p>
        <img src="files/FastRobots/Lab1/UUID.png">
        <h3>Prelab : Codebase</h3>
        <p>
            The connection between the Artemis and laptop was established using bluetooth. In a bluetooth 
            connection, there is a peripheral device and central device (the peripheral device stores the services and the central device
            views the services and gets data from the peripheral). The Artemis board is the peripheral device and the laptop is the central
            device.
        </p>
        <div style="background-color: black; margin: 10vh 100vw;"></div>
        <h3>Lab 1A Tasks</h3>
        <div class="text-row">
            <div class="left-column">
                <h3 style="color:white">Configuration</h3>
            </div>
            <div class="right-column">
                <p>The BAUD rate in all given example scripts and the serial montor was reset to 115200 so that the 
                    computer could communicate with the board consistently for all scripts.</p>
            </div>
        </div>
        <div class="text-row" style="background-color: white;">
            <div class="left-column">
                <h3>Task 1</h3>
                <p style="color:black">Running the example script blink to make the LED blink was successful.</p>
            </div>
            <div class="right-column" style="display: flex; align-items: center; justify-content: center;">
                <video height=300px controls>
                    <source src="files/FastRobots/Lab1/blink.mp4">
                </video>
            </div>
        </div>
        <div class="text-row" style="background-color: orangered;">
            <div class="left-column">
                <h3 style="color:black">Task 2</h3>
                <p style="color:white">Running the example script Example4_Serial to make the Serial Monitor echo inputs was successful.</p>
            </div>
            <div class="right-column" style="display: flex; align-items: center; justify-content: center;">
                <video height=300px controls>
                    <source src="files/FastRobots/Lab1/serial.mp4">
                </video>
            </div>
        </div>
        <div class="text-row">
            <div class="left-column">
                <h3 style="color:white;">Task 3</h3>
                <p>Running the example script Example2_analogRead to make the Serial Monitor display the thermistor output
                    was successful. The temperature reading goes from approx. 33600 to aprox. 33800 while being held to increase the temperature 
                   with body heat.</p>
            </div>
            <div class="right-column" style="display: flex; align-items: center; justify-content: center;">
                <video height=300px controls>
                    <source src="files/FastRobots/Lab1/analog_therm.mp4">
                </video>
            </div>
        </div>
        <div class="text-row" style="background-color: white;">
            <div class="left-column">
                <h3>Task 4</h3>
                <p style="color:black">Running the example script Example1_MicrophoneOutput was successful. The intent was to read the output of
                    the microphone sensor (high numbers for loud sounds, low numbers when there is little/no microphone input or abmient sound). 
                    The output increases from a few hundred to 21,000 after snapping in front of the board, demonstrating that this device works..</p>
            </div>
            <div class="right-column" style="display: flex; align-items: center; justify-content: center;">
                <video height=300px controls>
                    <source src="files/FastRobots/Lab1/sound.mp4">
                </video>
            </div>
        </div>

        <div style="background-color: black; margin: 10vh 100vw;"></div>

        <h3>Lab 1B Tasks</h3>
        <div class="text-row" style="background-color: orangered; width:98vw;">
            <div class="left-column">
                <h3 style="color:black">Configuration</h3>
            </div>
            <div class="right-column" style="width:65vw;">
                <p style="color:white">Lab 1B focused on implementing several functions allowing bluetooth communication between the computer 
                    and artemis board. The commands implemented are detailed below. The BAUD rate in all given example scripts and the serial montor was reset to 115200 so that the 
                    computer could communicate with the board consistently for all scripts. Furthermore, the MAC address and the UUID generated 
                    in the prelab were used and entered into their respective variables were necessary (the configuration.yaml file and in 
                    the ble_arduino.ino file).</p>
            </div>
        </div>
        <div class="text-row">
            <div class="left-column">
                <h3 style="color:white;">Task 1</h3>
                <p>This task was to use and create the ECHO command. Much of the outline was already created, 
                    so printing the given message "Robot says -> __input__ :)" required creating a print statement with that message. 
                    The code for the echo command and output are ultimately:</p>
            </div>
            <div class="right-column" style="display: flex; align-items: center; justify-content: center;flex-direction: column;">
                <img src="files/FastRobots/Lab1/ECHO_code.png" style="width:65vw;">
                <img src="files/FastRobots/Lab1/ECHO_output.png" style="width:65vw;">
            </div>
        </div>
        <div class="text-row" style="background-color: white;">
            <div class="left-column">
                <h3>Task 2</h3>
                <p style="color:black">This task was intended to create and use the SEND_THREE_FLOATS function. This function is 
                    intended to take 3 float inputs and extract those values in the Arduino sketch/ display them in the Serial 
                    Monitor. The code and result for the SEND_THREE_FLOATS command implemented was:</p>
            </div>
            <div class="right-column" style="display: flex; align-items: center; justify-content: center; flex-direction: column;">
                <img src="files/FastRobots/Lab1/THREEFLOATS_code.png" style="width:65vw;">
                <img src="files/FastRobots/Lab1/THREEFLOATS_output.png" style="width:65vw;">
            </div>
        </div>
        <div class="text-row" style="background-color: orangered;">
            <div class="left-column">
                <h3 style="color:black">Task 3</h3>
                <p style="color:white">The purpose of this task was to create the GET_TIME_MILLIS command. Unlike the previous 
                    2 tasks, this command did not already have skeleton code throughout the ble_arduino.ino and cmd_types.py file. 
                    The code implemented in ble_arduino.ino is shown in the topmost figure, followed by the time printed in the arduino 
                    serial monitor. The goal, however, was to send the time from the arduino to the computer over bluetooth. Therefore, the 
                    time output is also attached in the final figure.
                </p>
            </div>
            <div class="right-column" style="display: flex; align-items: center; justify-content: center;flex-direction: column;">
                <!-- Add right column  -->
                 <img src="files/FastRobots/Lab1/TIME_code.png" style="width:65vw;">
                 <img src="files/FastRobots/Lab1/TIME_output.png" style="width:65vw;">
                 <img src="files/FastRobots/Lab1/TIME_output2.png" style="width:65vw;">
            </div>
        </div>
        <div class="text-row">
            <div class="left-column">
                <h3 style="color:white;">Task 4</h3>
                <p>This task involved setting up a notification handler in python in the Jupyer Notebook. JUST the time portion 
                    of the string needs to be extracted. The code involved and output were:</p>
            </div>
            <div class="right-column" style="display: flex; align-items: center; justify-content: center;flex-direction: column;">
                <img src="files/FastRobots/Lab1/NOTIFICATION_code.png" style="width:65vw;">
                <img src="files/FastRobots/Lab1/NOTIFICATION_output.png" style="width:65vw;">
            </div>
        </div>
        <div class="text-row" style="background-color: white;">
            <div class="left-column">
                <h3>Task 5</h3>
                <p style="color:black">To determine how fast messages can be sent, a loop was created that would act over 5 seconds. The number of messages would be divided 
                    by 5 seconds for the final messages/second speed. The loop TIME_MESSAGE_LOOP was created the same way that the GET_TIME_MILLIS command was created 
                    (equal to 7 instead of 6 in cmd_types.py). The code for the loop is shown. The result was that 
                    the total number of messages printed by the serial monitor was 175. This means that (N=175)/5 messages were sent per second, 
                    which is 
                    equal to 35 messages/second.</p>
            </div>
            <div class="right-column" style="display: flex; align-items: center; justify-content: center; flex-direction: column;">
                <img src="files/FastRobots/Lab1/TIME_LOOP_code.png" style="width:65vw;">
            </div>
        </div>
        <div class="text-row" style="background-color: orangered;">
            <div class="left-column">
                <h3 style="color:black">Task 6</h3>
                <p style="color:white">In Task 5, the goal was to see how fast messages could be sent. A new strategy was attempted to make the messages 
                    faster in Task 6 in a new command SEND_TIME_DATA. All of the timestamps were generated and stored in an array "time_arr", and the messages in the 
                    array were sent afterwards. 
                    To calculate the speed of time stamps using this method of generation, the array size was pre-set to 10000 messages. Then the final time message value - the 
                    first time message value was used for delta t (change in time). Then the number of messages (10000)/ delta t gave the message rate per second. 
                    The code for the SEND_TIME_DATA section is as shown in the topmost figure in this section. After running SEND_TIME_DATA, the total time elapsed was 265 milliseconds. 10000 / 0.265 = 37735 time stamps generated per minute. 
                    This demonstrates that it is much more efficient to generate many messages, time stamps, or calculations as possible before sending messages, 
                    which takes longer.  Although it is not reasonable to display all 10000 messages, the lines of code used to run the command SEND_TIME_DATA and the first 
                    few results are shown in the second figure.
                </p>
            </div>
            <div class="right-column" style="display: flex; align-items: center; justify-content: center;flex-direction: column;">
                 <img src="files/FastRobots/Lab1/TIME_DATA_code.png" style="width:65vw;">
                 <img src="files/FastRobots/Lab1/TIME_DATA_output.png" style="width:65vw;">
            </div>
        </div>
        <div class="text-row">
            <div class="left-column">
                <h3 style="color:white;">Task 7</h3>
                <p>This task focused on using the same message strategy as Task 6, but for temperature data. All of the temperature readings 
                    were to be gathered first before sending the temperature data array. Each timestamp recorded in "time_arr" corresponds to the same index 
                    of "temp_arr". The format for time data is already "T:_time_", therefore the decided upon string to send time and temperature data was "T:_time_-C:_temperature_".
                    In otherwords, T preceeds the time, C preceeds the temperature in celcius, and a dash "-" separates the time and temperature.
                    The new command GET_TEMP_READINGS is used to collect this data and then loop through the arrays and send the data back. Furthermore, 
                    it did not make sense to have all of the temperature readings from less than a second for delta t; these measurements would be too similar. Therefore, a 
                    time delay was also added. The code to collect this data and add this time delay in GET_TEMP_READINGS is the topmost figure. 
                    The notification handler also had to be updated to handle the time AND temperature readings. It both still prints the data (specifying 
                    between time and temperature readings) and adds the time and temperature data into two respective lists. The updated notification handler code is the second 
                    figure. It is not feasible to share all of the temperature outputs due to the volume of them. The first few readings of the 
                    output of the code after using the GET_TEMP_READINGS command are in the third/ bottom figure. 
                </p>
            </div>
            <div class="right-column" style="display: flex; align-items: center; justify-content: center;flex-direction: column;">
                <img src="files/FastRobots/Lab1/TEMP_READINGS_code.png" style="width:65vw;">
                <img src="files/FastRobots/Lab1/TEMP_READINGS_code2.png" style="width:65vw;">
                <img src="files/FastRobots/Lab1/TEMP_READINGS_output.png" style="width:65vw;">
            </div>
        </div>
        <div class="text-row" style="background-color: white;">
            <div class="left-column">
                <h3>Task 8</h3>
            </div>
            <div class="right-column" style="display: flex; align-items: center; justify-content: center; flex-direction: column;">
                <p style="color:black">The advantage of sending data as it is aquired (NOT first put into an array) is that the data aquired is as relevant as possible to
                    when it is being recieved. Storing all of the measurements in an array first makes so that time has passed before recieving the data, and only 
                    the last measurements are likely going to be necessary. For times when computations need to be made in real time based on the current state of the board 
                    or robot it is attached to, it would be wise NOT to store lots of data in an array before sending it. The second method has the advantage of collecting 
                    more data per time period. According to the calculations in Task 6, 37735 time stamps were generated per minute suing this method. This is much greater than 
                    the 35 messages per minute sent in Task 5.
                    This increased time density and rate of transfer could be useful for complicated maneuvers. It would also be useful for 
                    controllers such as a PID where past data or 
                    analysis of trends is relevant. Another concern with the array method, however, is that the current lab 1 script already takes up 28% of the memory on the board. This 
                    lab is introductory, and not nearly as complicated as an actual robot would be. Therefore, it is important to consider that if this method is used, 
                    the memory constraints of the board are important. This concern can be mitigated by sending less of each kind of data (measurements over less time at 
                    the increased time density OR adding an artificial delay to decrease the measurements over the same period of time). To be exact in these memory calculations, 
                    the board has 384 kB of RAM. The floats for the time and temperature take up 4 bytes each. 384kB / 4 bytes = 96,000  floats that can be sent. These can 
                    be divided among different measurements (ie time or temperature), and also must be regulated by the aforementioned number of measurements over time.</p>
            </div>
        </div>

        <div style="background-color: black; margin: 10vh 100vw;"></div>

        <h2>Discussions and Conclusion</h2>
        <div style="width:98vw;">
            <p>
                One of the primary lessons involved in this lab was how to use bluetooth with the Artemis Nano at all. Another lesson the lab 
            focuses on which device sends data and when. Since the central device in a bluetooth connection recieves the data when relevant and the 
            peripheral device sends the data, it made sense through these activities that the Artemis was the peripheral deivce. It sent the time and 
            temperature data. Furthermore, it introduced the nuances of how to use the bluetooth. The robots throughout the course are intended to be very fast, 
            hence the name "Fast Robots", so it will be important to know how to send data as quickly and efficiently as possible from the Artemis board and any relevant sensors.
            </p>
        </div>
        
    </div>
    <div id="FastRobot_Lab2" class="subtabcontent">
        <div style="background-color: black; margin: 10vh 98vw;"></div>
        <div style="width:98vw;">
            <h2 style="color:white;">Lab 2</h2>
            <p>The purpose of this lab is to set up the IMU and test that it works (also implementing any equations related to make it work).
                The three sections of this lab are: Setting up the IMU (plugging it in and testing an example script), using the IMU 
                accelerometer, and using the IMU gyroscope.
            </p>
        </div>

        <div style="background-color: black; margin: 10vh 98vw;"></div>
        <div style="width:98vw;">
            <h3>General IMU Setup</h3>
            <p style="color: white;">
                The IMU was plugged in and the example code was run. In the example code, the AD0_VAL was also set to 1. This is the last bit of the I2C address, 
                which is dependent on if the 
                ADR jumper is closed (in this case it is closed). When testing the outputs of the values in the demo script, the gyroscope was 0
                when the IMU is stationary and increases as the velocity of the IMU increases. The accelerometer shows the acceleration in each 
                direction. The main board also blinks when the lab 2 script starts.
            </p>
        </div>
        <div class="text-row" style="width:98vw;">
            <img src="files/FastRobots/Lab2/IMU.jpg" style="height:70vh;">
            <img src="files/FastRobots/Lab2/ExampleCode.png" style="height:70vh;"> 
        </div>
        <div style="background-color: black; margin: 10vh 98vw;"></div>
        <h3>Accelerometer</h3>
        <p style="color:white;">
            The first step was to implement equations that would use accelerometer data to output an angle in degrees.
        </p>
        <div class="text-row">
            <img src="files/FastRobots/Lab2/AccelerometerEquations.png" style="width: 70%;">
        </div>
        <div style="background-color: black; margin: 5vh 98vw;"></div>
        <p>
            The accelerometer in the IMU using these equations was then tested. The input (manually twisting the IMU) vs the output of 
            the equations is shown below.
        </p>
        <div class="text-row"style="background-color:white;margin-bottom:0.5vh">
            <div class="left-column" style="width:50%;">
                <h2 style="color:orangered">Input</h2>
            </div>
            <div class="right-column" style="width:50%">
                <h2 style="color:orangered">Output</h2>
            </div>
        </div>
        <div class="text-row"style="background-color:white;margin-bottom:0.5vh">
            <div class="left-column" style="width:50%">
                <p style="color:black">Manual pitch of {-90,0,90}</p>
            </div>
            <div class="right-column" style="width:50%">
                <p style="color:black">
                    Pitch is:-82.01 <br>
                    Roll is:9.53 <br>
                    <br>
                    Pitch is:0.32 <br>
                    Roll is:-1.31 <br>
                    <br>
                    Pitch is:84.97 <br>
                    Roll is:16.03 <br>
                </p>
            </div>
        </div>
        <div class="text-row"style="background-color:white">
            <div class="left-column" style="width:50%">
                <p style="color:black">Manual roll of {-90,0,90}</p>
            </div>
            <div class="right-column" style="width:50%">
                <p style="color:black">
                    Pitch is:-1.17 <br>
                    Roll is:-84.60 <br>
                    <br>
                    Pitch is:0.32 <br>
                    Roll is:-1.31  <br>
                    <br>
                    Pitch is:4.40 <br>
                    Roll is:86.27 <br>
                </p>
            </div>
        </div>
        <p style="color:white">The accelerometer is about 2 to 10 degrees off. Additionally, there is an immediate spike in huge error as 
            soon as the IMU turns, but after a few seconds it returns to values close to the expected value. This is fine for the use case of 
            this current lab, but may be a problem when the IMU is on the car since it should be moving fast, which would mean it is not held 
            still very long in this use case. This would make the readings very innacurate on the robot. </p>
        <div style="background-color: black; margin: 5vh 98vw;"></div>
        <p style="color:white">Additionally, to analyse the noise, the following FFT was performed for both pitch and roll while the IMU was still: </p>
        <img src="files/FastRobots/Lab2/AccelerometerNoiseStill.png", width="400">
        <img src="files/FastRobots/Lab2/AccelerometerFFTStill.png", width="400">
        <div style="background-color: black; margin: 5vh 98vw;"></div>
        <p style="color:white">Then, the following FFT was performed for both pitch and roll while the IMU was rotated: </p> 
        <img src="files/FastRobots/Lab2/AccelerometerNoiseRotated.png", width="400">
        <img src="files/FastRobots/Lab2/AccelerometerFFTRotated.png", width="400">
        <p style="color:white">The only significant spike is up tp approximately 7 Hz, while the rest of the FFT appeared not to have 
            significant problems. The cutoff filter is then at 7 Hz. The data rate was aquired by using the number of messages sent 
            divided by the change in time, which is approximately 333 messages/second. The low pass filter code and results are attached below: </p>
            <img src="files/FastRobots/Lab2/LPFCode1.png">
            <img src="files/FastRobots/Lab2/LPFCode2.png">
            <div></div>
            <img src="files/FastRobots/Lab2/LPFResultsRaw.png", width="400">
            <img src="files/FastRobots/Lab2/LPFResultsFiltered.png", width="400">
            <div style="background-color: black; margin: 5vh 98vw;"></div>
            <p style="color:white">It can be clearly seen that the LPF decreases the noise significantly in these results. </p>

            <div style="background-color: black; margin: 5vh 98vw;"></div>
            <h3>Gyroscope</h3>
            <p style="color:white">The equations used for the gyroscope were: </p>
            <img src="files/FastRobots/Lab2/GyroscopeEquations.png">
            <div style="background-color: black; margin: 5vh 98vw;"></div>
            <p style="color:white">The following graph was obtained while keeping the pitch, roll, and yaw still over 15 seconds. 
                The error clearly grows very quickly since after being held still for these 15 seconds, the error is already between 5 and 10 degrees.
                When the sampling frequency was decreased, the accuracy increased. There were fewer measurements that were slightly off to 
                be added to the overall accumulated error. Despite this, there is also less noise in the gyroscope than the accelerometer. </p>
            <img src="files/FastRobots/Lab2/GyroscopeData1.png", width="400">
            <div style="background-color: black; margin: 5vh 98vw;"></div>
            <p style="color:white">The complementary filter was then added to sombine the accelerometer and gyroscope measurements. The alpha value 
                was set to 0.1 (the weight of the gyroscope). This value made the graphs most accurate when still after experimentation with multiple values. 
                The graph in this configuration is shown below, as well as the equation used for the filter: </p>
            <img src="files/FastRobots/Lab2/ComplementaryFilterEquation.png">
            <p style="color:white">  </p>
            <img src="files/FastRobots/Lab2/GyroscopeData2.png", width="400">
            <div style="background-color: black; margin: 5vh 98vw;"></div>

            <h3>Sample Data</h3>
            <p style="color:white">After removing all print statements and delays and sending all data over bluetooth, the message rate was 333.3 
                messages/second. Additionally, I used separate arrays for storage of different kinds of data. It would not take any less 
                storage space if all of the data was in one array, and variable names make it easier to access information. Furthermore, it makes 
                it easier to write functions for proccessing data, such as the low pass filter funciton, that can be applied to an entire 
                array at the same time as long as the necessary data to process is in its own array. This also means if only some of the data 
                is relevant, not all of it must be sent. For example, just the comp filter data can be sent and not the raw accelerometer 
                data if these intermediary values are not used. Floats were used for the comp data since there are computations that result in decimals 
                that need to be transferred. The floats are 4B, which is the same size as an int. Since there is no reason to sacrifice the capabilities 
                of a float without saving any data use, no ints were used. The total memory size is 384 kB, so in order to send LPF accelerometer data, 
                gyroscope data, comp data, and time data, 384kB/16B = 24567 measurements that can be sent. Using the 333.3 messages per second measured 
                earlier, 73.7 seconds worth of data can be sent. The code used to send the messages for these calculations was:</p>
            <img src="files/FastRobots/Lab2/SampleDataCode.png">
            <p style="color:white">Graphing this code results in the plot used for the complementary filter above. </p>
            <div style="background-color: black; margin: 5vh 98vw;"></div>

            <h3>Stunt</h3>
            <p style="color:white">The robot was driven in a hallway. The car does not drive perfectly straight, as seen in the video 
                below. Some steering compensation may have to be done for this in later use of the robot. </p>
            <video width="320" height="240" controls>
                <source src="files/FastRobots/Lab2/Stunt.mp4" type="video/mp4"> </video>


        </div>

    
    </div>
    
    <div id="MakeAThon" class="tabcontent">
        <!-- Add scrolling photo bar -->
        <!-- Project Overview -->
    </div>


    <script src="js/portfolio_scripts.js"></script>
</body>
</html>